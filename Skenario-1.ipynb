{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98719314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SKENARIO 1: BASE MODEL COMPARISON (ResNet50 Only)\n",
      "Pipeline: Scaler + Tuned LogReg (C=100)\n",
      "================================================================================\n",
      "\n",
      "üìÇ Memproses Fitur: ResNet50\n",
      "   Path File: Dataset CSV/resnet50_features(normalized).csv\n",
      "   üëâ [SINGLE] Training SVM... Done. (F1: 90.49%)\n",
      "   üëâ [SINGLE] Training Logistic Regression... Done. (F1: 90.75%)\n",
      "   üëâ [SINGLE] Training Decision Tree... Done. (F1: 76.42%)\n",
      "   üëâ [ENSEMBLE] Training Random Forest... Done. (F1: 87.86%)\n",
      "   üëâ [ENSEMBLE] Training XGBoost... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 163\u001b[39m\n\u001b[32m    160\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     \u001b[43mrun_scenario_1_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mrun_scenario_1_base_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   üëâ [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkategori\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m    116\u001b[39m y_pred = model.predict(X_test_scaled)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1806\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1786\u001b[39m evals_result: EvalsLog = {}\n\u001b[32m   1787\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1788\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1789\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1803\u001b[39m     feature_types=feature_types,\n\u001b[32m   1804\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1806\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1821\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:2434\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2433\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Matikan warning agar output bersih\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cek & Import Library Boosting (Ensemble)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    xgb = None\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except ImportError:\n",
    "    lgb = None\n",
    "\n",
    "# ================= KONFIGURASI PATH =================\n",
    "# UPDATE: Path disesuaikan dengan folder \"Dataset CSV\" dan nama file baru\n",
    "DATASETS = {\n",
    "    'ResNet50': 'Dataset CSV/resnet50_features(normalized).csv'\n",
    "}\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# ================= DEFINISI MODEL SESUAI FLOWCHART =================\n",
    "def get_models():\n",
    "    models = {}\n",
    "    \n",
    "    # --- KELOMPOK 1: SINGLE MODELS ---\n",
    "    \n",
    "    # 1. SVM (Wajib Scaler -> Sudah Aman)\n",
    "    models['SVM'] = SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # 2. LOGISTIC REGRESSION (TUNED)\n",
    "    # C=100.0 digunakan untuk mengimbangi efek Scaling agar performa tetap tinggi\n",
    "    models['Logistic Regression'] = LogisticRegression(\n",
    "        C=0.1,             \n",
    "        max_iter=1000, \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # 3. Decision Tree\n",
    "    models['Decision Tree'] = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "    \n",
    "    # --- KELOMPOK 2: ENSEMBLE MODELS ---\n",
    "    models['Random Forest'] = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE)\n",
    "    \n",
    "    if xgb is not None:\n",
    "        models['XGBoost'] = xgb.XGBClassifier(eval_metric='logloss', random_state=RANDOM_STATE)\n",
    "    \n",
    "    if lgb is not None:\n",
    "        models['LightGBM'] = lgb.LGBMClassifier(random_state=RANDOM_STATE, verbose=-1)\n",
    "        \n",
    "    return models\n",
    "\n",
    "# ================= FUNGSI UTAMA =================\n",
    "def run_scenario_1_base_model():\n",
    "    print(\"=\"*80)\n",
    "    print(\"SKENARIO 1: BASE MODEL COMPARISON (ResNet50 Only)\")\n",
    "    print(\"Pipeline: Scaler + Tuned LogReg (C=100)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    final_results = []\n",
    "\n",
    "    # 1. Loop Dataset (Hanya ResNet50)\n",
    "    for dataset_name, csv_path in DATASETS.items():\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"‚ö†Ô∏è File tidak ditemukan di path: {csv_path}\")\n",
    "            print(\"   Pastikan folder 'Dataset CSV' ada di lokasi yang sama dengan script ini.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüìÇ Memproses Fitur: {dataset_name}\")\n",
    "        print(f\"   Path File: {csv_path}\")\n",
    "        \n",
    "        # Load Data\n",
    "        df = pd.read_csv(csv_path)\n",
    "        feature_cols = [c for c in df.columns if c.startswith('feature_')]\n",
    "        X = df[feature_cols].values\n",
    "        y = df['label_encoded'].values\n",
    "        \n",
    "        # Split Data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scaling (Tetap dilakukan untuk memastikan distribusi optimal bagi SVM & LogReg)\n",
    "        # Walaupun nama filenya 'normalized', StandardScaler (Z-Score) seringkali\n",
    "        # lebih disukai SVM daripada sekadar MinMax (0-1).\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Ambil daftar model\n",
    "        models_dict = get_models()\n",
    "        \n",
    "        # 2. Loop Training Model\n",
    "        for model_name, model in models_dict.items():\n",
    "            kategori = \"ENSEMBLE\" if model_name in ['Random Forest', 'XGBoost', 'LightGBM'] else \"SINGLE\"\n",
    "            \n",
    "            print(f\"   üëâ [{kategori}] Training {model_name}...\", end=\" \")\n",
    "            \n",
    "            # Train\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Evaluasi\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            \n",
    "            print(f\"Done. (F1: {f1:.2%})\")\n",
    "            \n",
    "            # Simpan hasil\n",
    "            final_results.append({\n",
    "                'Feature Extractor': dataset_name,\n",
    "                'Category': kategori,\n",
    "                'Model': model_name,\n",
    "                'Accuracy': acc,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec,\n",
    "                'F1-Score': f1\n",
    "            })\n",
    "\n",
    "    # ================= OUTPUT HASIL =================\n",
    "    if final_results:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üèÜ HASIL SKENARIO 1: PERBANDINGAN MODEL (BASE)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df_results = pd.DataFrame(final_results)\n",
    "        \n",
    "        # Urutkan berdasarkan F1-Score tertinggi\n",
    "        df_results = df_results.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Format angka\n",
    "        output_table = df_results.copy()\n",
    "        for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
    "            output_table[col] = output_table[col].map('{:.2%}'.format)\n",
    "            \n",
    "        print(output_table.to_string(index=False))\n",
    "        \n",
    "        # Highlight Juara\n",
    "        best = df_results.iloc[0]\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(f\"ü•á BEST MODEL: {best['Model']}\")\n",
    "        print(f\"   Score: {best['F1-Score']:.2%}\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_scenario_1_base_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
